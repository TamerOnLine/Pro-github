{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here's what this code does in detail:\n\nWhat is the Boston Housing Dataset?\nThe Boston Housing dataset is a famous dataset from the 1970s that provides data for housing prices in the Boston suburbs. It contains:\n\n13 features describing various attributes of the houses (e.g., crime rate, number of rooms, distance to employment centers, etc.)\n1 target variable which is the median value of owner-occupied homes in thousands of dollars.","metadata":{}},{"cell_type":"markdown","source":"# Importing the Dataset","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import boston_housing","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:02.220757Z","iopub.execute_input":"2024-09-23T11:49:02.221242Z","iopub.status.idle":"2024-09-23T11:49:18.296649Z","shell.execute_reply.started":"2024-09-23T11:49:02.221181Z","shell.execute_reply":"2024-09-23T11:49:18.295404Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"This line of code imports the Boston Housing dataset from the **tensorflow.keras.datasets** module, which contains many built-in datasets for training and testing machine learning models.","metadata":{}},{"cell_type":"markdown","source":"# Loading the Dataset","metadata":{}},{"cell_type":"markdown","source":"After importing, you typically load the dataset as follows:","metadata":{}},{"cell_type":"code","source":"# Load the dataset\n(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.299002Z","iopub.execute_input":"2024-09-23T11:49:18.299680Z","iopub.status.idle":"2024-09-23T11:49:18.591445Z","shell.execute_reply.started":"2024-09-23T11:49:18.299634Z","shell.execute_reply":"2024-09-23T11:49:18.590098Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n\u001b[1m57026/57026\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2us/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"-   **train_data**: Training data with features.\n-   **train_targets**: Corresponding prices (target values) for the training set.\n-   **test_data**: Testing data with features.\n-   **test_targets**: Corresponding prices (target values) for the testing set.\n\n    -   The dataset is split into training and test sets so that you can train your model on one part of the data and evaluate it on a separate part to check its generalization performance.","metadata":{}},{"cell_type":"code","source":"# Print dataset shapes\nprint(f\"Training data shape: {train_data.shape}\")\nprint(f\"Test data shape: {test_data.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.592688Z","iopub.execute_input":"2024-09-23T11:49:18.593279Z","iopub.status.idle":"2024-09-23T11:49:18.599567Z","shell.execute_reply.started":"2024-09-23T11:49:18.593223Z","shell.execute_reply":"2024-09-23T11:49:18.598248Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Training data shape: (404, 13)\nTest data shape: (102, 13)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This will load the Boston Housing dataset and print out the shape of the training and test sets. You can then use this dataset to train models for regression tasks, where the goal is to predict housing prices based on the features.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"   The code provided is calculating the mean and standard deviation for the training data in the Boston Housing dataset. These statistical metrics are often used to normalize or standardize data before training machine learning models. Here's a detailed breakdown:","metadata":{}},{"cell_type":"code","source":"mean = train_data.mean(axis=0)\nstd = train_data.std(axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.602493Z","iopub.execute_input":"2024-09-23T11:49:18.603006Z","iopub.status.idle":"2024-09-23T11:49:18.613237Z","shell.execute_reply.started":"2024-09-23T11:49:18.602951Z","shell.execute_reply":"2024-09-23T11:49:18.611577Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"1. train_data.mean(axis=0):\n\n    -   This line computes the mean of the training data along the columns (features).\n    -   axis=0 means that the operation is applied across the rows, meaning the mean is computed for each feature (column) in the training dataset.\n    -   The result is a 1D array where each element is the mean of one feature.\n\n2.  train_data.std(axis=0):\n\n    -   This line computes the standard deviation of the training data for each feature.\n    -   The standard deviation gives a measure of the spread or dispersion of the data points from the mean for each feature.\n    -   Like the mean, this is computed across the rows for each feature, resulting in a 1D array where each element is the standard deviation of one feature.\n\n**Why Compute Mean and Standard Deviation?**\n-   In machine learning, especially with datasets like the Boston Housing dataset, features often have very different scales. For instance, one feature might represent a number of rooms (which could range from 3 to 10), while another might represent distance to employment centers (which could range from 1 to 30). These different scales can make it harder for many models to converge during training.\n\n-   To address this, we often standardize the data by:\n\n    -   Subtracting the mean from each feature, so it becomes centered around 0.\n    -   Dividing by the standard deviation, so each feature has a standard deviation of 1.\n    \nThis process helps many machine learning algorithms perform better by ensuring that each feature contributes equally to the model's learning process.","metadata":{}},{"cell_type":"markdown","source":"This step will ensure that all features have a mean of 0 and a standard deviation of 1, making the data more suitable for training models like neural networks, linear regression, or other algorithms that are sensitive to the scale of input features.\n\n**Summary**\n\n-   Mean: Helps center the data around 0.\n-   Standard deviation: Scales the data so that each feature has the same importance.\n-   Normalization: Ensures the data is scaled consistently across features, improving model performance.","metadata":{}},{"cell_type":"markdown","source":"   This line of code is performing normalization of the test data using the mean and standard deviation calculated from the training set. Here's an explanation of the code and the reasoning behind it:","metadata":{}},{"cell_type":"code","source":"test_data = (test_data - mean) / std","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.615118Z","iopub.execute_input":"2024-09-23T11:49:18.615665Z","iopub.status.idle":"2024-09-23T11:49:18.625311Z","shell.execute_reply.started":"2024-09-23T11:49:18.615605Z","shell.execute_reply":"2024-09-23T11:49:18.623969Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"-   test_data: This is the dataset that will be used to evaluate the performance of the trained model. It's separate from the training data to ensure the model is tested on unseen data.\n-   mean: This is the mean of each feature in the training data, calculated earlier.\n-   std: This is the standard deviation of each feature in the training data, also calculated earlier.\n\n**Why Normalize Test Data Using Training Statistics?**\n\n-   Training data statistics (mean and standard deviation) are used for normalizing both the training and test data. This ensures consistency, as the test data should be transformed in the same way as the training data for accurate evaluation.\n-   Do not use test data statistics: Using the mean and standard deviation of the test data directly would leak information about the test set into the model, leading to over-optimistic results. To properly evaluate the model's generalization, the test data must remain unseen during the training process, and it should be normalized using only the statistics from the training data.\n\n**Normalization Process:**\n-   Subtract the mean: Each feature in the test data is shifted so that it has the same mean (calculated from the training set) as the training data, which helps center the data around zero.\n-   Divide by the standard deviation: This step scales each feature in the test data so that the spread or variability matches that of the training data, ensuring that all features are on the same scale.","metadata":{}},{"cell_type":"markdown","source":"By following this approach, you ensure that both the training and test sets are on the same scale, which helps the model perform well when it's evaluated on unseen data.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# models.Sequential()\n\n In this step, you're preparing to create a machine learning model using TensorFlow's Keras API by importing the necessary modules for building and configuring a neural network. Here's a detailed breakdown of what you're doing and how to proceed.","metadata":{}},{"cell_type":"code","source":"# Model creation using TensorFlow Keras\nfrom tensorflow.keras import models, layers","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.626989Z","iopub.execute_input":"2024-09-23T11:49:18.627456Z","iopub.status.idle":"2024-09-23T11:49:18.638545Z","shell.execute_reply.started":"2024-09-23T11:49:18.627400Z","shell.execute_reply":"2024-09-23T11:49:18.637210Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"-   **models**: This module is used to create and configure models. You can define your neural network architecture, including how layers are connected, the input and output, and how to compile the model.\n\n-   **layers**: This module contains various types of layers that you can use to build your neural network. Common examples include Dense, Dropout, Conv2D, etc. Layers are the building blocks of a neural network.","metadata":{}},{"cell_type":"markdown","source":"## 1. Initialize the Model","metadata":{}},{"cell_type":"code","source":"# Build the model\nmodel = models.Sequential()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.639870Z","iopub.execute_input":"2024-09-23T11:49:18.640302Z","iopub.status.idle":"2024-09-23T11:49:18.653403Z","shell.execute_reply.started":"2024-09-23T11:49:18.640258Z","shell.execute_reply":"2024-09-23T11:49:18.652235Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"-   **models.Sequential()**: This initializes a Sequential model, which allows you to stack layers one after another in a linear fashion. It’s the simplest type of model in Keras, suitable for most feed-forward neural networks.","metadata":{}},{"cell_type":"markdown","source":"## 2. Adding the First Hidden Layer","metadata":{}},{"cell_type":"markdown","source":"In the updated code, you're explicitly adding an input layer using layers.Input. Here's a detailed explanation of the changes:","metadata":{}},{"cell_type":"markdown","source":"### 1. Explicit Input Layer:","metadata":{}},{"cell_type":"code","source":"model.add(layers.Input(shape=(train_data.shape[1],)))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.654677Z","iopub.execute_input":"2024-09-23T11:49:18.655043Z","iopub.status.idle":"2024-09-23T11:49:18.668880Z","shell.execute_reply.started":"2024-09-23T11:49:18.655005Z","shell.execute_reply":"2024-09-23T11:49:18.667458Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"-   **layers.Input()**: This explicitly adds an input layer to the model. It specifies the shape of the input data that the model will accept.\n-   **shape=(train_data.shape[1],)**: This specifies that the input data will have a shape equal to the number of features in the dataset. For the Boston Housing dataset, train_data.shape[1] is 13, so this input layer expects data with 13 features.","metadata":{}},{"cell_type":"markdown","source":"### 2. First Dense Layer:","metadata":{}},{"cell_type":"code","source":"model.add(layers.Dense(64, activation='relu'))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.670427Z","iopub.execute_input":"2024-09-23T11:49:18.670854Z","iopub.status.idle":"2024-09-23T11:49:18.754483Z","shell.execute_reply.started":"2024-09-23T11:49:18.670803Z","shell.execute_reply":"2024-09-23T11:49:18.753110Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"-   This is the first fully connected (dense) layer with 64 neurons and ReLU activation.\n-   The model will apply the ReLU function to each neuron's output, introducing non-linearity to help the model learn more complex patterns.","metadata":{}},{"cell_type":"markdown","source":"**When Should You Use layers.Input?**\n\nExplicit input layers are more commonly used in functional API models or complex architectures with multiple inputs or outputs. In a simple sequential model like this, adding an explicit Input layer is optional, but it can make the code clearer, especially if the model is likely to become more complex later.","metadata":{}},{"cell_type":"markdown","source":"**Summary**:\n\n-   **layers.Input(shape=(train_data.shape[1],))**: Explicitly defines the input layer, specifying that the model expects data with 13 features (in the Boston Housing dataset).\n-   **layers.Dense(64, activation='relu')**: Adds a fully connected layer with 64 units and ReLU activation, which is the first hidden layer of the model.\n\n**This approach gives you more control over your model architecture, and it’s especially useful when dealing with more sophisticated models.**","metadata":{}},{"cell_type":"markdown","source":"## 3. Adding the Second Hidden Layer","metadata":{}},{"cell_type":"code","source":"model.add(layers.Dense(64, activation='relu'))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.757869Z","iopub.execute_input":"2024-09-23T11:49:18.758307Z","iopub.status.idle":"2024-09-23T11:49:18.776538Z","shell.execute_reply.started":"2024-09-23T11:49:18.758263Z","shell.execute_reply":"2024-09-23T11:49:18.775458Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"-   This adds another fully connected layer with 64 neurons and ReLU activation. This layer will take the outputs from the previous layer as its input.","metadata":{}},{"cell_type":"markdown","source":"## 4. Adding the Output Layer","metadata":{}},{"cell_type":"code","source":"model.add(layers.Dense(1))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.777786Z","iopub.execute_input":"2024-09-23T11:49:18.778169Z","iopub.status.idle":"2024-09-23T11:49:18.799382Z","shell.execute_reply.started":"2024-09-23T11:49:18.778109Z","shell.execute_reply":"2024-09-23T11:49:18.798217Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"-   **layers.Dense(1)**: The output layer has 1 neuron because the model is solving a regression problem, where the goal is to predict a single continuous value (house price). There’s no activation function here because the output should be a continuous number, not a classification or probability.\n\n**Full Explanation of the Model**\n-   The model starts with an input layer of 13 features (from the Boston Housing dataset).\n-   It has two hidden layers, each with 64 neurons and ReLU activation, allowing the network to learn non-linear relationships between the features and the target.\n-   The output layer has one neuron, as it is a regression problem, where the network outputs the predicted house price.\n\n**Summary**\n\n-   This model is structured to handle a regression task (predicting housing prices) using a simple feed-forward neural network with two hidden layers of 64 neurons each. The ReLU activation helps the network learn complex patterns, and the single output neuron provides the final predicted house price.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# model.summary()\n\nThe **model.summary()** function provides a detailed summary of the model's architecture, showing information about the layers, output shape, number of parameters, and total trainable parameters. This is a great way to inspect the model before training to ensure everything is set up correctly.\n\nHere’s what happens when you call model.summary():","metadata":{}},{"cell_type":"code","source":"# Print model summary\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.800720Z","iopub.execute_input":"2024-09-23T11:49:18.801104Z","iopub.status.idle":"2024-09-23T11:49:18.825836Z","shell.execute_reply.started":"2024-09-23T11:49:18.801062Z","shell.execute_reply":"2024-09-23T11:49:18.824625Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m896\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,121\u001b[0m (20.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,121</span> (20.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,121\u001b[0m (20.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,121</span> (20.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"Explanation of Each Section:\n\n-   **Layer (type)**:\n\nThis column lists each layer in your model and its type (in this case, Dense for fully connected layers).\n\n-**Output Shape**:\n\nThis column shows the shape of the output tensor for each layer. The shape is described in terms of batches of data, so the first dimension is usually None (indicating that the model can handle any batch size), and the second dimension corresponds to the number of units in that layer.\n\n**Param**:\n\nThis column displays the number of parameters (weights and biases) that the layer has. For a dense layer, the number of parameters is \n    \n**calculated as:(input_features×units)+units**\n\n**Where**:\n\n-   **input_features** is the number of inputs to the layer (i.e., the number of neurons in the previous layer).\n-   **units** is the number of neurons in the current layer.","metadata":{}},{"cell_type":"markdown","source":"**Total Params**:\n\nThis shows the total number of trainable parameters (i.e., the weights and biases that the model will adjust during training). \n\n**Why model.summary() is useful:**\n\n-   It provides a quick and clear overview of the model architecture.\n-   It shows the number of parameters, which is important to understand the complexity of the model.\n-   It helps to catch issues (like incorrect layer configurations or mismatched input shapes) before starting training.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# model.compile()\n\nThe line **model.compile(optimizer='adam', loss='mse', metrics=['mae'])** is preparing the model for training by configuring the optimizer, loss function, and metrics to be tracked. Here’s a detailed explanation of each part:","metadata":{}},{"cell_type":"code","source":"# Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss='mse',\n    metrics=['mae'])","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.827189Z","iopub.execute_input":"2024-09-23T11:49:18.827552Z","iopub.status.idle":"2024-09-23T11:49:18.845937Z","shell.execute_reply.started":"2024-09-23T11:49:18.827515Z","shell.execute_reply":"2024-09-23T11:49:18.844713Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## 1. Optimizer: adam","metadata":{}},{"cell_type":"markdown","source":"-   **Adam** (Adaptive Moment Estimation) is an optimization algorithm that computes adaptive learning rates for each parameter. It combines the advantages of two other popular optimizers:\n    -   **Momentum**: Keeps track of an exponentially decaying average of past gradients to smooth the optimization.\n    -   **RMSProp**: Adapts the learning rate based on the recent magnitude of gradients.\n-   **Adam** is widely used because it is computationally efficient, has little memory requirements, and works well in practice with little tuning.","metadata":{}},{"cell_type":"markdown","source":"## 2. Loss Function: mse (Mean Squared Error)","metadata":{}},{"cell_type":"markdown","source":"-   Mean Squared Error (**MSE**) is a common loss function for regression problems, where the goal is to predict a continuous value.\nMSE calculates the average of the squared differences between the predicted values and the actual values","metadata":{}},{"cell_type":"markdown","source":"## 3. Metrics: mae (Mean Absolute Error)","metadata":{}},{"cell_type":"markdown","source":"-   Mean Absolute Error (**MAE**) is a performance metric that measures the average of the absolute differences between the predicted and actual values","metadata":{}},{"cell_type":"markdown","source":"**This compiles the model with:**\n\n-   **Adam** optimizer: For efficient gradient-based optimization.\n-   **MSE** loss function: To minimize the squared difference between actual and predicted values, perfect for regression.\n-   **MAE** metric: To track the average absolute error during training and evaluation.\n\n**Summary**:\n\n-   **optimizer='adam'**: Optimizes the model’s parameters using the Adam algorithm.\n-   **loss='mse'**: Minimizes the mean squared error between the predicted and actual values.\n-   **metrics=['mae']**: Tracks the mean absolute error, giving a clearer sense of how the model’s predictions differ from actual values.\n\n    -   This is an essential step before training the model. After this, the model is ready to be trained using the **.fit()** function.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# model.fit()\n\nThe provided code is used to train the model on the training data. Here's an explanation of each part:","metadata":{}},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(train_data, train_targets, \n                    epochs=100, \n                    batch_size=16, \n                    validation_split=0.2)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:18.848003Z","iopub.execute_input":"2024-09-23T11:49:18.848510Z","iopub.status.idle":"2024-09-23T11:49:30.257237Z","shell.execute_reply.started":"2024-09-23T11:49:18.848455Z","shell.execute_reply":"2024-09-23T11:49:30.255745Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 2491.3130 - mae: 40.3893 - val_loss: 245.7039 - val_mae: 12.7296\nEpoch 2/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 233.1046 - mae: 12.3000 - val_loss: 96.4305 - val_mae: 6.3049\nEpoch 3/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66.9572 - mae: 6.4339 - val_loss: 88.4938 - val_mae: 5.8566\nEpoch 4/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.3907 - mae: 4.9467 - val_loss: 69.9403 - val_mae: 5.9274\nEpoch 5/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58.0772 - mae: 5.2777 - val_loss: 77.2593 - val_mae: 5.4776\nEpoch 6/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 50.0558 - mae: 4.8888 - val_loss: 74.3685 - val_mae: 5.3768\nEpoch 7/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68.3396 - mae: 5.4618 - val_loss: 68.9615 - val_mae: 5.2554\nEpoch 8/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.8544 - mae: 5.0636 - val_loss: 66.4628 - val_mae: 5.2094\nEpoch 9/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.5073 - mae: 4.4656 - val_loss: 64.3676 - val_mae: 5.3937\nEpoch 10/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58.2906 - mae: 4.9557 - val_loss: 62.8584 - val_mae: 5.3254\nEpoch 11/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 50.4047 - mae: 5.2038 - val_loss: 63.9939 - val_mae: 5.1438\nEpoch 12/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 57.9197 - mae: 5.3295 - val_loss: 60.5757 - val_mae: 5.3762\nEpoch 13/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63.1638 - mae: 5.7623 - val_loss: 62.9403 - val_mae: 5.0948\nEpoch 14/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56.3112 - mae: 5.1026 - val_loss: 69.0729 - val_mae: 5.1658\nEpoch 15/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 45.3417 - mae: 4.5364 - val_loss: 61.9852 - val_mae: 5.9066\nEpoch 16/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53.7715 - mae: 5.1632 - val_loss: 57.6246 - val_mae: 5.1593\nEpoch 17/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.6772 - mae: 5.0108 - val_loss: 74.8911 - val_mae: 5.4659\nEpoch 18/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62.1565 - mae: 5.2027 - val_loss: 58.0378 - val_mae: 4.8075\nEpoch 19/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 42.1745 - mae: 4.5637 - val_loss: 57.5677 - val_mae: 4.7670\nEpoch 20/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 39.8789 - mae: 4.2201 - val_loss: 71.8640 - val_mae: 5.3225\nEpoch 21/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34.1554 - mae: 4.1944 - val_loss: 56.9775 - val_mae: 4.6414\nEpoch 22/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.8941 - mae: 4.5193 - val_loss: 54.9388 - val_mae: 4.6522\nEpoch 23/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43.2640 - mae: 4.4942 - val_loss: 53.2438 - val_mae: 5.3675\nEpoch 24/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.9519 - mae: 4.5637 - val_loss: 69.4206 - val_mae: 5.2693\nEpoch 25/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 49.1198 - mae: 4.5029 - val_loss: 54.7425 - val_mae: 4.4833\nEpoch 26/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.5093 - mae: 4.1240 - val_loss: 52.7869 - val_mae: 5.4516\nEpoch 27/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 50.3259 - mae: 5.1693 - val_loss: 99.3377 - val_mae: 7.2815\nEpoch 28/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55.6411 - mae: 5.0817 - val_loss: 67.7464 - val_mae: 5.2914\nEpoch 29/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.5982 - mae: 4.0535 - val_loss: 47.8106 - val_mae: 4.4808\nEpoch 30/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 46.0114 - mae: 4.7584 - val_loss: 55.1434 - val_mae: 4.6903\nEpoch 31/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 38.9833 - mae: 4.3609 - val_loss: 48.4630 - val_mae: 4.3053\nEpoch 32/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34.4252 - mae: 4.0488 - val_loss: 84.2470 - val_mae: 6.5768\nEpoch 33/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55.0170 - mae: 5.0810 - val_loss: 48.8148 - val_mae: 4.2639\nEpoch 34/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.0084 - mae: 4.0890 - val_loss: 48.0358 - val_mae: 4.4465\nEpoch 35/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 30.3651 - mae: 4.0396 - val_loss: 42.0720 - val_mae: 4.4052\nEpoch 36/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.2552 - mae: 4.4184 - val_loss: 56.8812 - val_mae: 6.2452\nEpoch 37/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48.4147 - mae: 5.3806 - val_loss: 41.2063 - val_mae: 4.4715\nEpoch 38/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 32.3312 - mae: 4.2399 - val_loss: 40.8935 - val_mae: 4.3421\nEpoch 39/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.5257 - mae: 4.3032 - val_loss: 49.5386 - val_mae: 5.6810\nEpoch 40/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 59.1113 - mae: 6.1940 - val_loss: 44.6714 - val_mae: 4.1538\nEpoch 41/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 30.3948 - mae: 3.8303 - val_loss: 59.6975 - val_mae: 5.2236\nEpoch 42/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 37.1265 - mae: 4.2395 - val_loss: 37.5486 - val_mae: 4.3004\nEpoch 43/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.3627 - mae: 3.8324 - val_loss: 37.5113 - val_mae: 4.0725\nEpoch 44/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.2220 - mae: 3.5961 - val_loss: 35.7046 - val_mae: 4.1316\nEpoch 45/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34.5115 - mae: 4.2128 - val_loss: 37.3539 - val_mae: 4.3609\nEpoch 46/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.8905 - mae: 4.2099 - val_loss: 36.5751 - val_mae: 3.8851\nEpoch 47/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31.8593 - mae: 3.9840 - val_loss: 35.2523 - val_mae: 4.4902\nEpoch 48/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.9760 - mae: 4.2610 - val_loss: 35.5338 - val_mae: 3.9024\nEpoch 49/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.6206 - mae: 3.9138 - val_loss: 52.3632 - val_mae: 4.9259\nEpoch 50/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 37.8981 - mae: 4.4890 - val_loss: 60.3366 - val_mae: 5.4424\nEpoch 51/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48.0469 - mae: 4.8786 - val_loss: 53.6471 - val_mae: 4.9655\nEpoch 52/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.0010 - mae: 3.8647 - val_loss: 39.0157 - val_mae: 4.0167\nEpoch 53/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.6251 - mae: 3.6759 - val_loss: 40.4725 - val_mae: 4.0249\nEpoch 54/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31.0408 - mae: 4.1923 - val_loss: 33.6754 - val_mae: 4.0558\nEpoch 55/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34.9708 - mae: 4.0866 - val_loss: 60.4026 - val_mae: 5.6994\nEpoch 56/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 29.6499 - mae: 4.3290 - val_loss: 32.3737 - val_mae: 4.1990\nEpoch 57/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27.1962 - mae: 3.6926 - val_loss: 33.4101 - val_mae: 4.3735\nEpoch 58/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.6327 - mae: 4.0604 - val_loss: 41.4983 - val_mae: 4.3338\nEpoch 59/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 22.8379 - mae: 3.4610 - val_loss: 32.5477 - val_mae: 4.0551\nEpoch 60/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27.1020 - mae: 3.6455 - val_loss: 32.5835 - val_mae: 4.2908\nEpoch 61/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31.6565 - mae: 3.9464 - val_loss: 33.0091 - val_mae: 3.6511\nEpoch 62/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.4102 - mae: 3.5353 - val_loss: 35.0459 - val_mae: 4.7163\nEpoch 63/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.1288 - mae: 3.8301 - val_loss: 46.0107 - val_mae: 5.8321\nEpoch 64/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.0756 - mae: 4.7132 - val_loss: 33.5089 - val_mae: 4.5493\nEpoch 65/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25.3572 - mae: 3.8724 - val_loss: 35.7695 - val_mae: 4.8003\nEpoch 66/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 38.0239 - mae: 4.7139 - val_loss: 34.2872 - val_mae: 3.8183\nEpoch 67/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.4390 - mae: 3.2427 - val_loss: 37.5697 - val_mae: 4.0807\nEpoch 68/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.1830 - mae: 3.5527 - val_loss: 35.2104 - val_mae: 3.8672\nEpoch 69/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.5583 - mae: 3.5520 - val_loss: 29.1257 - val_mae: 3.8366\nEpoch 70/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25.0993 - mae: 3.5447 - val_loss: 31.3836 - val_mae: 3.6538\nEpoch 71/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.3708 - mae: 3.2537 - val_loss: 28.8427 - val_mae: 3.8434\nEpoch 72/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31.9062 - mae: 4.0487 - val_loss: 28.9062 - val_mae: 3.7278\nEpoch 73/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36.3839 - mae: 4.6584 - val_loss: 38.3964 - val_mae: 5.1324\nEpoch 74/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.2809 - mae: 4.0959 - val_loss: 33.8849 - val_mae: 4.8111\nEpoch 75/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.9263 - mae: 3.3804 - val_loss: 31.0908 - val_mae: 4.3553\nEpoch 76/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.8291 - mae: 3.7613 - val_loss: 31.6758 - val_mae: 4.5447\nEpoch 77/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26.5048 - mae: 3.8812 - val_loss: 27.7125 - val_mae: 3.6333\nEpoch 78/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.4494 - mae: 3.6027 - val_loss: 33.0167 - val_mae: 4.0583\nEpoch 79/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 30.7550 - mae: 4.0191 - val_loss: 31.5451 - val_mae: 3.7484\nEpoch 80/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 31.4580 - mae: 4.0763 - val_loss: 27.9717 - val_mae: 3.8976\nEpoch 81/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.4951 - mae: 3.5392 - val_loss: 36.8237 - val_mae: 5.0530\nEpoch 82/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.8197 - mae: 3.7042 - val_loss: 28.9754 - val_mae: 4.2278\nEpoch 83/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26.0018 - mae: 3.8996 - val_loss: 30.2325 - val_mae: 3.6784\nEpoch 84/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.1398 - mae: 3.5392 - val_loss: 27.4836 - val_mae: 3.8089\nEpoch 85/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.9943 - mae: 3.5611 - val_loss: 27.5780 - val_mae: 4.2311\nEpoch 86/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.1244 - mae: 3.4013 - val_loss: 27.9327 - val_mae: 3.5605\nEpoch 87/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.3306 - mae: 3.0401 - val_loss: 30.1614 - val_mae: 3.6532\nEpoch 88/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.9966 - mae: 3.7040 - val_loss: 26.5981 - val_mae: 3.5007\nEpoch 89/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.9847 - mae: 3.9834 - val_loss: 32.7248 - val_mae: 4.7945\nEpoch 90/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.1617 - mae: 3.9760 - val_loss: 33.6928 - val_mae: 3.9222\nEpoch 91/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 31.1889 - mae: 3.9347 - val_loss: 25.5939 - val_mae: 3.7850\nEpoch 92/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 21.4904 - mae: 3.4111 - val_loss: 31.4283 - val_mae: 4.6994\nEpoch 93/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.0255 - mae: 4.0512 - val_loss: 30.6726 - val_mae: 3.7288\nEpoch 94/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 26.6181 - mae: 3.7174 - val_loss: 25.3208 - val_mae: 3.4552\nEpoch 95/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.0830 - mae: 3.1437 - val_loss: 24.5711 - val_mae: 3.5211\nEpoch 96/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.0724 - mae: 4.6305 - val_loss: 53.8203 - val_mae: 6.3194\nEpoch 97/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.1533 - mae: 5.0038 - val_loss: 26.0649 - val_mae: 3.9971\nEpoch 98/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.0318 - mae: 3.4533 - val_loss: 26.1124 - val_mae: 3.5395\nEpoch 99/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.5206 - mae: 2.7824 - val_loss: 25.6402 - val_mae: 3.5126\nEpoch 100/100\n\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 24.0572 - mae: 3.6221 - val_loss: 30.1235 - val_mae: 3.6480\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1. Training the Model:\n\n    -   **model.fit()**: This is the method that trains the model on the training data.\n\n2. Arguments Explained:\n\n    -   **train_data**: This is the input data (the features of the Boston Housing dataset). It's the dataset that the model will use to learn.\n\n    -   **train_targets**: This is the target data (the house prices) that the model will try to predict. The model adjusts its parameters (weights and biases) based on the difference between its predictions and these actual values.\n\n    -   **epochs=100**: The number of times the model will go through the entire training dataset. In this case, it will train for 100 epochs. More epochs generally lead to better learning, but too many can cause overfitting.\n\n    -   **batch_size=16**: The number of training examples used in one forward/backward pass of training. Instead of updating the model’s weights after seeing every example, we update them after processing 16 examples. Smaller batch sizes tend to make training more stable, but larger batch sizes can be more efficient.\n\n    -   **validation_split=0.2**: This reserves 20% of the training data to be used as validation data. The model will be trained on the remaining 80%, and after each epoch, it will evaluate its performance on this 20% validation set. This allows you to monitor how well the model generalizes to unseen data and helps prevent overfitting.\n\n3. Training Process:\n\n**When this code runs**:\n\n-   The model will train on 80% of the training data and evaluate itself on the remaining 20% (as the validation set).\n-   For each epoch, it updates the weights and biases by going through batches of 16 examples.\n-   The model's loss and performance (both on the training data and validation data) are tracked over each epoch.\n\n4. Return Value (history):\n-   The **history** object contains the training metrics for each epoch, including the loss and the metrics you specified (**mae** in this case). You can use this **history** object later to visualize the training and validation loss, mean absolute error, and how the model's performance evolves over time.","metadata":{}},{"cell_type":"markdown","source":"**Summary**:\n\n-   The **model.fit()** function trains the model for 100 epochs using a batch size of 16.\n-   It reserves 20% of the training data for validation.\n-   The **history** object tracks the training and validation loss/metrics over time, allowing you to visualize the learning process","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# model.evaluate()\n\nThe code provided is evaluating the model's performance on the test data. Here's a breakdown of each part:","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test data","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:30.259831Z","iopub.execute_input":"2024-09-23T11:49:30.260481Z","iopub.status.idle":"2024-09-23T11:49:30.269805Z","shell.execute_reply.started":"2024-09-23T11:49:30.260430Z","shell.execute_reply":"2024-09-23T11:49:30.268107Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## 1. Evaluating the Model:","metadata":{}},{"cell_type":"code","source":"test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:30.271953Z","iopub.execute_input":"2024-09-23T11:49:30.272494Z","iopub.status.idle":"2024-09-23T11:49:30.380699Z","shell.execute_reply.started":"2024-09-23T11:49:30.272439Z","shell.execute_reply":"2024-09-23T11:49:30.379238Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 534.8087 - mae: 21.7126 \n","output_type":"stream"}]},{"cell_type":"markdown","source":"-   **model.evaluate()**: This method computes the loss and metrics (defined during \n    **model.compile())** for the provided test data. It runs a forward pass through the model with the test data, computes the loss (in this case, Mean Squared Error), and evaluates the chosen metric (Mean Absolute Error).\n\n-   **test_data**: This is the test dataset that the model has never seen during training, used to evaluate its generalization ability.\n\n-   **test_targets**: These are the actual target values (house prices) for the test dataset.\n\n-   Output:\n\n    -   The function returns two values:\n        -   **test_mse_score**: The Mean Squared Error on the test data (the default loss function we compiled the model with).\n        -   **test_mae_score**: The Mean Absolute Error on the test data, which provides an intuitive sense of the average error between the model's predictions and the actual values.","metadata":{}},{"cell_type":"markdown","source":"## 2. Printing the Result:","metadata":{}},{"cell_type":"code","source":"print(f\"Mean Absolute Error on test data: {test_mae_score}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:30.382552Z","iopub.execute_input":"2024-09-23T11:49:30.382954Z","iopub.status.idle":"2024-09-23T11:49:30.390846Z","shell.execute_reply.started":"2024-09-23T11:49:30.382904Z","shell.execute_reply":"2024-09-23T11:49:30.388804Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Mean Absolute Error on test data: 22.0926570892334\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This prints out the Mean Absolute Error (MAE) on the test dataset, which is a useful metric to evaluate the model's performance. It gives you an idea of how far off, on average, the model’s predictions are from the actual house prices.","metadata":{}},{"cell_type":"markdown","source":"**Summary**:\n\n-   **model.evaluate()** runs the model on the test data and returns the test loss and any additional metrics (in this case, MAE).\n-   **Mean Absolute Error** is useful for understanding how much the model’s predictions deviate from the actual target values, providing a tangible measure of the model’s accuracy.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Matplotlib\n\n**The provided Python code is using the matplotlib library to plot the training and validation loss of a machine learning model over a number of training epochs. Here's a detailed breakdown of each part:**","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Matplotlib","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:30.392790Z","iopub.execute_input":"2024-09-23T11:49:30.393317Z","iopub.status.idle":"2024-09-23T11:49:30.404028Z","shell.execute_reply.started":"2024-09-23T11:49:30.393272Z","shell.execute_reply":"2024-09-23T11:49:30.402826Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"This line imports the pyplot module from the matplotlib library, which is used for creating static, animated, and interactive visualizations in Python. The alias plt is commonly used for convenience.","metadata":{}},{"cell_type":"markdown","source":"## 2. Extracting Loss Values","metadata":{}},{"cell_type":"code","source":"history_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:30.405449Z","iopub.execute_input":"2024-09-23T11:49:30.405840Z","iopub.status.idle":"2024-09-23T11:49:30.418939Z","shell.execute_reply.started":"2024-09-23T11:49:30.405801Z","shell.execute_reply":"2024-09-23T11:49:30.417751Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Here, we assume history is the result of training a machine learning model, possibly with a library like TensorFlow/Keras. The history object contains training metrics, and .history is a dictionary where:\n\n-   'loss' corresponds to the training loss values (the error during training) across each epoch.\n-   'val_loss' corresponds to the validation loss values (the error on the validation set) across each epoch.","metadata":{}},{"cell_type":"markdown","source":"## 3. Defining the Epoch Range","metadata":{}},{"cell_type":"code","source":"epochs = range(1, len(loss_values) + 1)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:30.420499Z","iopub.execute_input":"2024-09-23T11:49:30.421000Z","iopub.status.idle":"2024-09-23T11:49:30.435088Z","shell.execute_reply.started":"2024-09-23T11:49:30.420941Z","shell.execute_reply":"2024-09-23T11:49:30.433664Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The number of epochs refers to the number of full cycles through the training data during model training. This line defines a range of integers starting from 1 up to the total number of epochs (the length of the loss values array).","metadata":{}},{"cell_type":"markdown","source":"## 4. Plotting the Loss Values","metadata":{}},{"cell_type":"markdown","source":"~~~\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n~~~","metadata":{}},{"cell_type":"markdown","source":"This is where the plotting happens:\n\n-   The first plt.plot() call creates a line plot for the training loss (loss_values). 'bo' means \"blue circles\" as the marker for the data points.\n-   The second plt.plot() call creates a line plot for the validation loss (val_loss_values). 'b' is used here for a solid blue line.\n-   The label parameter in both plots provides a legend description for each dataset (training and validation loss).","metadata":{}},{"cell_type":"markdown","source":"## 5. Adding Titles and Labels","metadata":{}},{"cell_type":"markdown","source":"~~~\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n~~~\n\n","metadata":{}},{"cell_type":"markdown","source":"These lines set the title of the plot, and label the x-axis (which represents the epochs) and the y-axis (which represents the loss values).","metadata":{}},{"cell_type":"markdown","source":"## 6. Adding a Legend","metadata":{}},{"cell_type":"markdown","source":"~~~\nplt.legend()\n~~~","metadata":{}},{"cell_type":"markdown","source":"This displays the legend on the plot, using the label values provided earlier for the training and validation loss curves.","metadata":{}},{"cell_type":"markdown","source":"## 7. Displaying the Plot","metadata":{}},{"cell_type":"code","source":"# 4\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n\n# 5\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\n# 6\nplt.legend()\n\n# 7\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T11:49:30.436900Z","iopub.execute_input":"2024-09-23T11:49:30.437421Z","iopub.status.idle":"2024-09-23T11:49:31.059256Z","shell.execute_reply.started":"2024-09-23T11:49:30.437363Z","shell.execute_reply":"2024-09-23T11:49:31.057951Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"This function is used to display the plot. In certain environments, like Jupyter Notebooks, the plot may automatically appear without needing this function. However, in other environments (like some code editors or when running the script from the command line), it is necessary to call plt.show() to actually render and display the plot.\n\nWithout this command, the plot might not appear, or the program may finish execution before the plot is rendered. Therefore, it is an essential step to ensure the plot is visible at the end of the script.\n\n**Adding plt.show() at the end ensures the plot will be displayed correctly after all the data has been plotted.**","metadata":{}},{"cell_type":"markdown","source":"Output\nThe resulting plot will show two curves:\n\n-   Training loss (as blue circles along the curve) for each epoch.\n-   Validation loss (as a solid blue line) for each epoch.\n\nThis plot helps visualize how the model's error (or loss) changes during training and how well the model generalizes (validation loss) across epochs.","metadata":{}}]}